# -*- coding: utf-8 -*-
"""AndrewTetteh_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-7BbTzm55HKPad-xeF6m6dQThHjaNy0T
"""

#librairies to use
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

#loading datasets for the training and testing
fifa_training = pd.read_csv('/content/male_players (legacy).csv', na_values="-")
fifa_training.head()

fifa_testing = pd.read_csv('/content/players_22-1.csv', na_values="-")
fifa_testing.head()

#looking for missing datasets
fifa_training.isnull().sum()

fifa_testing.isnull().sum()

numeric_data = fifa_training.select_dtypes(include=[np.number])
categorical_data = fifa_training.select_dtypes(include=["object"])

fifa_training.info()

fifa_testing.info()

fifa_testing.shape

fifa_training.shape

fifa_testing.describe()

fifa_training.describe()

# dropping the rows in 'overall' column
fifa_training.dropna(subset=['overall'], inplace=True)

# important features for analysis
important_features = ['overall', 'potential', 'value_eur', 'wage_eur', 'age', 'height_cm', 'weight_kg',
                      'mentality_composure', 'defending_marking_awareness', 'defending_standing_tackle',
                      'defending_sliding_tackle', 'goalkeeping_diving', 'goalkeeping_handling',
                      'goalkeeping_kicking', 'goalkeeping_positioning', 'goalkeeping_reflexes', 'goalkeeping_speed']

#regrouping the dataset with important features
fifa_training = fifa_training[important_features]
fifa_testing = fifa_testing[important_features]

# the correction of the dataset and the graphical representation
fifa_training_corr = fifa_training.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(fifa_training_corr, annot=True)
plt.title('FIFA Ratings Correlation')
plt.show()

#imputing missing values for both training and testing data
imputer = SimpleImputer(strategy='mean')
X = fifa_training.drop(columns=['overall'])
y = fifa_training['overall']

X_imputed = imputer.fit_transform(X)
X = pd.DataFrame(X_imputed, columns=X.columns)

X_test = fifa_testing.drop(columns=['overall'])
X_test_imputed = imputer.transform(X_test)
X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)
y_test = fifa_testing['overall']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# initializing machine learning models I will be using for regression
training_models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'KNeighbors': KNeighborsRegressor(),
    'Support Vector Machine': SVR()
}

# training and testing the models using k-fold cross-validation
results = {}
for name, model in training_models.items():
    cross_validation = cross_val_score(model, X_train, y_train, cv=10, scoring='r2')
    results[name] = cross_validation.mean()
    print(f'{name}: Mean R2 Score = {results[name]}')

#trraining and evaluating the models
for name, model in training_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    print(f'{name} Root Mean Squared Error: {rmse}')

#selecting the best model based on cross-validation results
model_name = max(results, key=results.get)
best_model = training_models[model_name]

print(f'Best Model: {model_name}')

#hyperparameter tuning for the best model to use
if model_name in ['Random Forest', 'Gradient Boosting', 'KNeighbors']:
    hyperparameter_grid = {}
    grid_search = GridSearchCV(best_model, hyperparameter_grid, cv=5, scoring='r2')
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_

    print(f'Best Model with Grid Search: {best_model}')

#testing the best model
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
test_rmse = mean_squared_error(y_test, y_pred, squared=False)
test_r2 = r2_score(y_test, y_pred)

print(f'The Root Mean Squared Error: {test_rmse}')
print(f'R2 Score: {test_r2}')

!pip install streamlit
import streamlit as st
st.title('Rating Prediction')
st.sidebar.header('Input')

User_inputs = {}
for feature in important_features[1:]:
    User_inputs[feature] = st.sidebar.number_input(f'Enter {feature}', value=float(X[feature].mean()))

# Prediction function
def rating_pred(User_inputs):
    input_data = pd.DataFrame([User_inputs])
    input_imputed = imputer.transform(input_data)
    prediction = best_model.predict(input_imputed)
    return prediction[0]

# Prediction and display
if st.sidebar.button('Predict the Rating'):
    prediction = rating_pred(User_inputs)
    st.write(f'Predicted Rating: {prediction:.2f}')

#testint the test data
y_test_pred = best_model.predict(X_test)
test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)
test_r2 = r2_score(y_test, y_test_pred)

print(f'Root Mean Squared Error: {test_rmse}')
print(f'R2 Score: {test_r2}')